# Influence-of-Artificial-Degrees-of-Certainty-in-Large-Language-Model-Reasoning-Based-Tasks
## Project Rationale
As large language models (LLMs) are increasingly integrated into various decision-making systems (such as medical diagnosis and customer service), it becomes imperative to understand the factors that influence their performance. Prior research have demonstrated that human inputs within a prompt fed to LLMs, such as specific instructions (the obvious case), subtle biases, or tone of voice, can significantly shape produced responses. This project examines the impact of one particular human input: when evaluating the performance of LLMs for reasoning tasks, the prompt consists of an additional human-generated degree of certainty attached to some answer (that could be right or wrong). By varying the attached degree of certainty, we investigate how such human input can impact the accuracy and bias of LLM responses.

Due to the ever-expanding applications of LLMs in addressing real-world problems, the mitigation of incorrect outputs is a key priority. Thus, this project aims to shed light on potential vulnerabilities of LLM behavior, and the findings of our research could contribute to methods for enhancing LLM reliability, reducing AI misinformation, and improving human-AI collaboration.
